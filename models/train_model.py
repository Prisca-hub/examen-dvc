#!/usr/bin/env python3
"""
Script d'entraÃ®nement du modÃ¨le avec les meilleurs paramÃ¨tres
"""

import pandas as pd
import pickle
import os
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import numpy as np

print("ï¿½ï¿½ Chargement des donnÃ©es et des meilleurs paramÃ¨tres...")

# Charger les donnÃ©es
X_train_scaled = pd.read_csv('data/processed_data/X_train_scaled.csv')
X_test_scaled = pd.read_csv('data/processed_data/X_test_scaled.csv')
y_train = pd.read_csv('data/processed_data/y_train.csv').values.ravel()
y_test = pd.read_csv('data/processed_data/y_test.csv').values.ravel()

print(f"X_train_scaled shape : {X_train_scaled.shape}")
print(f"X_test_scaled shape : {X_test_scaled.shape}")
print(f"y_train shape : {y_train.shape}")
print(f"y_test shape : {y_test.shape}")

# Charger le meilleur modÃ¨le du GridSearch
try:
    with open('models/best_model.pkl', 'rb') as f:
        best_model = pickle.load(f)
    print(f"âœ… Meilleur modÃ¨le chargÃ© : {type(best_model).__name__}")
except FileNotFoundError:
    print("âŒ Fichier best_model.pkl non trouvÃ©. Lancez d'abord gridsearch.py")
    exit(1)

# Charger les meilleurs paramÃ¨tres
try:
    with open('models/best_params.pkl', 'rb') as f:
        best_params = pickle.load(f)
    print(f"âœ… Meilleurs paramÃ¨tres : {best_params}")
except FileNotFoundError:
    print("âš ï¸ Fichier best_params.pkl non trouvÃ©")
    best_params = {}

print("\nğŸš€ EntraÃ®nement du modÃ¨le final...")

# Le modÃ¨le est dÃ©jÃ  entraÃ®nÃ© depuis le GridSearch, mais on peut le rÃ©entraÃ®ner
# avec tous les donnÃ©es d'entraÃ®nement pour Ãªtre sÃ»r
final_model = best_model
final_model.fit(X_train_scaled, y_train)

print("âœ… ModÃ¨le entraÃ®nÃ© !")

print("\nğŸ“Š Ã‰valuation du modÃ¨le...")

# PrÃ©dictions
y_pred_train = final_model.predict(X_train_scaled)
y_pred_test = final_model.predict(X_test_scaled)

# MÃ©triques d'Ã©valuation
train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
train_mae = mean_absolute_error(y_train, y_pred_train)
test_mae = mean_absolute_error(y_test, y_pred_test)
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)

# Afficher les rÃ©sultats
print("ğŸ“ˆ MÃ©triques de performance :")
print(f"   Train RMSE : {train_rmse:.4f}")
print(f"   Test RMSE  : {test_rmse:.4f}")
print(f"   Train MAE  : {train_mae:.4f}")
print(f"   Test MAE   : {test_mae:.4f}")
print(f"   Train RÂ²   : {train_r2:.4f}")
print(f"   Test RÂ²    : {test_r2:.4f}")

# CrÃ©er un dictionnaire avec toutes les mÃ©triques
metrics = {
    'train_rmse': train_rmse,
    'test_rmse': test_rmse,
    'train_mae': train_mae,
    'test_mae': test_mae,
    'train_r2': train_r2,
    'test_r2': test_r2,
    'model_type': type(final_model).__name__,
    'best_params': best_params
}

print("\nğŸ’¾ Sauvegarde du modÃ¨le final...")

# Sauvegarder le modÃ¨le final entraÃ®nÃ©
with open('models/final_model.pkl', 'wb') as f:
    pickle.dump(final_model, f)

# Sauvegarder les mÃ©triques
with open('models/model_metrics.pkl', 'wb') as f:
    pickle.dump(metrics, f)

# Sauvegarder les prÃ©dictions pour analyse
predictions = {
    'y_train_true': y_train,
    'y_train_pred': y_pred_train,
    'y_test_true': y_test,
    'y_test_pred': y_pred_test
}

with open('models/predictions.pkl', 'wb') as f:
    pickle.dump(predictions, f)

print("âœ… EntraÃ®nement terminÃ© !")
print("ğŸ“ Fichiers sauvÃ©s :")
print("   - models/final_model.pkl (modÃ¨le entraÃ®nÃ©)")
print("   - models/model_metrics.pkl (mÃ©triques)")
print("   - models/predictions.pkl (prÃ©dictions)")

print(f"\nğŸ¯ RÃ©sumÃ© final :")
print(f"   ModÃ¨le : {type(final_model).__name__}")
print(f"   RÂ² Test : {test_r2:.4f}")
print(f"   RMSE Test : {test_rmse:.4f}")

# VÃ©rifier si le modÃ¨le a de bonnes performances
if test_r2 > 0.8:
    print("ğŸ‰ Excellent modÃ¨le ! (RÂ² > 0.8)")
elif test_r2 > 0.6:
    print("ğŸ‘ Bon modÃ¨le (RÂ² > 0.6)")
else:
    print("âš ï¸ ModÃ¨le Ã  amÃ©liorer (RÂ² < 0.6)")
